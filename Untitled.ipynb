{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8a60fc8-d541-4f76-9161-ea58ed080c05",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain_groq import ChatGroq\n",
    "from langchain.chains import ConversationalRetrievalChain\n",
    "from langchain_groq import ChatGroq\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccaa2942-109d-44e4-89dd-896e5a137455",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import get_groq_api_key\n",
    "groq_api_key = get_groq_api_key()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a7db4da6-4f83-4940-91a7-f99947fcb580",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 236 pages from PDF's\n"
     ]
    }
   ],
   "source": [
    "# Loading the PDF's\n",
    "pdf=['fastfacts-what-is-climate-change.pdf','thebook.pdf']\n",
    "all_docs=[]\n",
    "\n",
    "for path in pdf:\n",
    "    loader= PyPDFLoader(path)\n",
    "    data=loader.load()\n",
    "    all_docs.extend(data)\n",
    "\n",
    "print(f\"Loaded {len(all_docs)} pages from PDF's\")    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d66d4dff-6326-4d69-a98a-836258e4860a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split in 566 chunks.\n"
     ]
    }
   ],
   "source": [
    "# Splitting Text into chunks\n",
    "\n",
    "text_splitter= RecursiveCharacterTextSplitter(chunk_size= 1000,chunk_overlap=200)\n",
    "chunks=text_splitter.split_documents(all_docs)\n",
    "print(f\"Split in {len(chunks)} chunks.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0e442a08-9ec9-477c-924b-c98058a005d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FAISS created successfully in 27.88 seconds \n"
     ]
    }
   ],
   "source": [
    "# Create Embedding and storing it in Vector DB\n",
    "embeddings= HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "start= time.time()\n",
    "db=FAISS.from_documents(chunks,embeddings)\n",
    "endtime= time.time() - start\n",
    "print(f\"FAISS created successfully in {endtime:.2f} seconds \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5504719-afdc-4166-a970-f7065456da5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatGroq(\n",
    "    model=\"llama-3.1-8b-instant\",  \n",
    "    temperature=0.2,\n",
    "    max_tokens=512,                \n",
    "    api_key=groq_api_key\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "764d6f3b-d6bc-41d9-ac54-a5dc57355cc4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chat system is ready\n"
     ]
    }
   ],
   "source": [
    "# Creating Conversational Chain\n",
    "chain= ConversationalRetrievalChain.from_llm(\n",
    "    llm=llm,\n",
    "    retriever=db.as_retriever(),\n",
    "    return_source_documents=True\n",
    ")\n",
    "print(\"Chat system is ready\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "97283187-d18a-4d74-99bd-d5472d62135e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Ask a question (or type 'exit' to stop):  exit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "👋 Chat Ended\n"
     ]
    }
   ],
   "source": [
    "# Making Chat loop\n",
    "\n",
    "Chat_history=[]\n",
    "\n",
    "while True:\n",
    "    question = input(\"\\nAsk a question (or type 'exit' to stop): \")\n",
    "\n",
    "    if question.lower() == \"exit\":\n",
    "        print(\"👋 Chat Ended\")\n",
    "        break\n",
    "\n",
    "    response = chain({\"question\": question, \"chat_history\": Chat_history})\n",
    "    answer = response[\"answer\"]\n",
    "\n",
    "    print(f\"\\n🧠 Your Answer: {answer}\")\n",
    "    Chat_history.append((question, answer))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "93c30c1c-e4dd-4f59-842a-a88fbdf07be1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Chat_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cf902d1-b211-4bef-ad91-c5bd5412feb6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Arbaz Aslam\\AppData\\Local\\Temp\\ipykernel_19184\\2061928366.py:68: UserWarning: You have not specified a value for the `type` parameter. Defaulting to the 'tuples' format for chatbot messages, but this is deprecated and will be removed in a future version of Gradio. Please set type='messages' instead, which uses openai-style dictionaries with 'role' and 'content' keys.\n",
      "  chatbot = gr.Chatbot(label=\"Chat with your PDFs\", height=400)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7865\n",
      "* To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7865/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:    Exception in ASGI application\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Arbaz Aslam\\AppData\\Roaming\\Python\\Python313\\site-packages\\uvicorn\\protocols\\http\\h11_impl.py\", line 403, in run_asgi\n",
      "    result = await app(  # type: ignore[func-returns-value]\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "        self.scope, self.receive, self.send\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "    )\n",
      "    ^\n",
      "  File \"C:\\Users\\Arbaz Aslam\\AppData\\Roaming\\Python\\Python313\\site-packages\\uvicorn\\middleware\\proxy_headers.py\", line 60, in __call__\n",
      "    return await self.app(scope, receive, send)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\Arbaz Aslam\\AppData\\Roaming\\Python\\Python313\\site-packages\\fastapi\\applications.py\", line 1082, in __call__\n",
      "    await super().__call__(scope, receive, send)\n",
      "  File \"C:\\Users\\Arbaz Aslam\\AppData\\Roaming\\Python\\Python313\\site-packages\\starlette\\applications.py\", line 113, in __call__\n",
      "    await self.middleware_stack(scope, receive, send)\n",
      "  File \"C:\\Users\\Arbaz Aslam\\AppData\\Roaming\\Python\\Python313\\site-packages\\starlette\\middleware\\errors.py\", line 186, in __call__\n",
      "    raise exc\n",
      "  File \"C:\\Users\\Arbaz Aslam\\AppData\\Roaming\\Python\\Python313\\site-packages\\starlette\\middleware\\errors.py\", line 164, in __call__\n",
      "    await self.app(scope, receive, _send)\n",
      "  File \"C:\\Users\\Arbaz Aslam\\AppData\\Roaming\\Python\\Python313\\site-packages\\gradio\\brotli_middleware.py\", line 74, in __call__\n",
      "    return await self.app(scope, receive, send)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\Arbaz Aslam\\AppData\\Roaming\\Python\\Python313\\site-packages\\gradio\\route_utils.py\", line 882, in __call__\n",
      "    await self.app(scope, receive, send)\n",
      "  File \"C:\\Users\\Arbaz Aslam\\AppData\\Roaming\\Python\\Python313\\site-packages\\starlette\\middleware\\exceptions.py\", line 63, in __call__\n",
      "    await wrap_app_handling_exceptions(self.app, conn)(scope, receive, send)\n",
      "  File \"C:\\Users\\Arbaz Aslam\\AppData\\Roaming\\Python\\Python313\\site-packages\\starlette\\_exception_handler.py\", line 53, in wrapped_app\n",
      "    raise exc\n",
      "  File \"C:\\Users\\Arbaz Aslam\\AppData\\Roaming\\Python\\Python313\\site-packages\\starlette\\_exception_handler.py\", line 42, in wrapped_app\n",
      "    await app(scope, receive, sender)\n",
      "  File \"C:\\Users\\Arbaz Aslam\\AppData\\Roaming\\Python\\Python313\\site-packages\\starlette\\routing.py\", line 716, in __call__\n",
      "    await self.middleware_stack(scope, receive, send)\n",
      "  File \"C:\\Users\\Arbaz Aslam\\AppData\\Roaming\\Python\\Python313\\site-packages\\starlette\\routing.py\", line 736, in app\n",
      "    await route.handle(scope, receive, send)\n",
      "  File \"C:\\Users\\Arbaz Aslam\\AppData\\Roaming\\Python\\Python313\\site-packages\\starlette\\routing.py\", line 290, in handle\n",
      "    await self.app(scope, receive, send)\n",
      "  File \"C:\\Users\\Arbaz Aslam\\AppData\\Roaming\\Python\\Python313\\site-packages\\starlette\\routing.py\", line 78, in app\n",
      "    await wrap_app_handling_exceptions(app, request)(scope, receive, send)\n",
      "  File \"C:\\Users\\Arbaz Aslam\\AppData\\Roaming\\Python\\Python313\\site-packages\\starlette\\_exception_handler.py\", line 53, in wrapped_app\n",
      "    raise exc\n",
      "  File \"C:\\Users\\Arbaz Aslam\\AppData\\Roaming\\Python\\Python313\\site-packages\\starlette\\_exception_handler.py\", line 42, in wrapped_app\n",
      "    await app(scope, receive, sender)\n",
      "  File \"C:\\Users\\Arbaz Aslam\\AppData\\Roaming\\Python\\Python313\\site-packages\\starlette\\routing.py\", line 75, in app\n",
      "    response = await f(request)\n",
      "               ^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\Arbaz Aslam\\AppData\\Roaming\\Python\\Python313\\site-packages\\fastapi\\routing.py\", line 308, in app\n",
      "    raw_response = await run_endpoint_function(\n",
      "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "    ...<3 lines>...\n",
      "    )\n",
      "    ^\n",
      "  File \"C:\\Users\\Arbaz Aslam\\AppData\\Roaming\\Python\\Python313\\site-packages\\fastapi\\routing.py\", line 219, in run_endpoint_function\n",
      "    return await dependant.call(**values)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\Arbaz Aslam\\AppData\\Roaming\\Python\\Python313\\site-packages\\gradio\\routes.py\", line 1664, in get_upload_progress\n",
      "    await asyncio.wait_for(\n",
      "        file_upload_statuses.is_tracked(upload_id), timeout=3\n",
      "    )\n",
      "  File \"G:\\Anaconda\\Lib\\asyncio\\tasks.py\", line 507, in wait_for\n",
      "    return await fut\n",
      "           ^^^^^^^^^\n",
      "  File \"C:\\Users\\Arbaz Aslam\\AppData\\Roaming\\Python\\Python313\\site-packages\\gradio\\route_utils.py\", line 528, in is_tracked\n",
      "    return await self._signals[upload_id].wait()\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"G:\\Anaconda\\Lib\\asyncio\\locks.py\", line 210, in wait\n",
      "    fut = self._get_loop().create_future()\n",
      "          ~~~~~~~~~~~~~~^^\n",
      "  File \"G:\\Anaconda\\Lib\\asyncio\\mixins.py\", line 20, in _get_loop\n",
      "    raise RuntimeError(f'{self!r} is bound to a different event loop')\n",
      "RuntimeError: <asyncio.locks.Event object at 0x000001B7B2E35190 [unset]> is bound to a different event loop\n",
      "C:\\Users\\Arbaz Aslam\\AppData\\Local\\Temp\\ipykernel_19184\\2061928366.py:49: LangChainDeprecationWarning: The method `Chain.__call__` was deprecated in langchain 0.1.0 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  result = chat_chain({\"question\": message, \"chat_history\": chat_history})\n"
     ]
    }
   ],
   "source": [
    "import gradio as gr\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain_groq import ChatGroq\n",
    "from langchain.chains import ConversationalRetrievalChain\n",
    "\n",
    "import os\n",
    "os.environ[\"GROQ_API_KEY\"] = \"GROQ_API_KEY\"\n",
    "\n",
    "chat_chain = None\n",
    "chat_history = []\n",
    "\n",
    "# 📘 Function to process uploaded PDF(s)\n",
    "def process_pdfs(pdf_files):\n",
    "    global chat_chain, chat_history\n",
    "\n",
    "    all_docs = []\n",
    "    for pdf in pdf_files:\n",
    "        loader = PyPDFLoader(pdf.name)\n",
    "        docs = loader.load()\n",
    "        all_docs.extend(docs)\n",
    "\n",
    "    splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=80)\n",
    "    chunks = splitter.split_documents(all_docs)\n",
    "\n",
    "    embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "    db = FAISS.from_documents(chunks, embeddings)\n",
    "\n",
    "    llm = ChatGroq(model=\"llama-3.1-8b-instant\", temperature=0.2)\n",
    "    chat_chain = ConversationalRetrievalChain.from_llm(\n",
    "        llm=llm,\n",
    "        retriever=db.as_retriever(search_kwargs={\"k\": 3}),\n",
    "        return_source_documents=True\n",
    "    )\n",
    "\n",
    "    chat_history = []\n",
    "    return \"✅ PDF processed successfully! You can now ask questions.\"\n",
    "\n",
    "\n",
    "# 💬 Function to chat with the processed PDFs\n",
    "def chat_with_pdfs(message, history):\n",
    "    global chat_chain, chat_history\n",
    "\n",
    "    if chat_chain is None:\n",
    "        return history + [[message, \"⚠️ Please upload and process a PDF first!\"]]\n",
    "\n",
    "    result = chat_chain({\"question\": message, \"chat_history\": chat_history})\n",
    "    answer = result[\"answer\"]\n",
    "\n",
    "    chat_history.append((message, answer))\n",
    "    history.append([message, answer])\n",
    "    return history\n",
    "\n",
    "\n",
    "# 🖥️ Build Gradio App\n",
    "with gr.Blocks(title=\"Chat with PDF\") as demo:\n",
    "    gr.Markdown(\"# 🤖 Chat with Your PDF\")\n",
    "    gr.Markdown(\"Upload one or more PDF files, process them, and ask questions interactively.\")\n",
    "\n",
    "    with gr.Row():\n",
    "        pdf_files = gr.File(label=\"📂 Upload PDF(s)\", file_count=\"multiple\", file_types=[\".pdf\"])\n",
    "        process_button = gr.Button(\"⚙️ Process PDFs\")\n",
    "\n",
    "    status_box = gr.Textbox(label=\"Status\", interactive=False)\n",
    "\n",
    "    chatbot = gr.Chatbot(label=\"Chat with your PDFs\", height=400)\n",
    "    msg = gr.Textbox(label=\"💬 Ask a question\", placeholder=\"Type here and press Enter...\")\n",
    "    clear = gr.Button(\"🧹 Clear Chat\")\n",
    "\n",
    "    # Event bindings\n",
    "    process_button.click(process_pdfs, inputs=[pdf_files], outputs=[status_box])\n",
    "    msg.submit(chat_with_pdfs, [msg, chatbot], [chatbot])\n",
    "    clear.click(lambda: None, None, chatbot, queue=False)\n",
    "\n",
    "demo.launch()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcb56a75-3adc-4177-a7e9-830ff7cc3f79",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
